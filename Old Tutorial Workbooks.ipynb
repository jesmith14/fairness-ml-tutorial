{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** OLD STUFF FROM PREVIOUS VERSIONS OF THE TUTORIAL / SOLUTION***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Data from Federal Reserve:\n",
    "#### Explanations / Guidelines:\n",
    "- All files are saved in current directory\n",
    "- We will be analyzing \"Figure_7.A._TransRisk_Score_Cumulative_Percentage_of_Goods_and_Bads,_by_Demographic_Group(Random-Account_Performance)_-_Race_or_ethnicity_(SSA_data).csv\" and saving that file as 'random-account-ficoscores.csv' for easier reference\n",
    "- To analyze the Cumulative Percentage of Goods and Bads for any of the other protected groups (sex, age, marital status, or income ratio) simply plug in this csv when assigning the 'data' variable\n",
    "- Options for account types: any-account, new-account, existing-account, random-account\n",
    "- In this study, \"good\" means non-defaulting for loans (will pay it off). \"Bad\" means defaulting for loans (will not pay it off)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps:\n",
    "\n",
    "Now that you have calculated the precision and recall of a single threshold, let's calculate it for every TransRisk score threshold. Create a pandas dataframe that includes all four groups (defaulting and non-defaulting) and TransRisk scores. With each score acting as the <i>threshold</i> value, for each group - determine the F1 score for that threshold. Are there any significant discrepancies you notice between groups?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getPrecisionSeries(goodName, badName, dataArray):\n",
    "    tp = dataArray[goodName]\n",
    "    fp = dataArray[badName]\n",
    "    precision = tp / (tp + fp)\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getRecallSeries(goodName, badName, dataArray):\n",
    "    totalGood = totalData[goodName].sum()\n",
    "    tp = dataArray[goodName]\n",
    "    recall = tp / totalGood\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getF1Series(goodName, badName, dataArray):\n",
    "    precision = getPrecisionSeries(goodName, badName, dataArray)\n",
    "    recall = getRecallSeries(goodName, badName, dataArray)\n",
    "    denom = ((1/precision) + (1/recall)) / 2\n",
    "    f1 = 1/denom\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "F1Scores = totalData[\"Score\"].to_frame(name=\"TransRisk Score\")\n",
    "white_series = getF1Series(\"Non- Hispanic white (Good)\", \"Non- Hispanic white (Bad)\", totalData)\n",
    "asian_series = getF1Series(\"Asian (Good)\", \"Asian (Bad)\", totalData)\n",
    "black_series = getF1Series(\"Black (Good)\", \"Black (Bad)\", totalData)\n",
    "hispanic_series = getF1Series(\"Hispanic (Good)\", \"Hispanic (Bad)\", totalData)\n",
    "F1Scores['White F1'] = white_series\n",
    "F1Scores['Asian F1'] = asian_series\n",
    "F1Scores['Black F1'] = black_series\n",
    "F1Scores['Hispanic F1'] = hispanic_series\n",
    "F1Scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Still trying to fix this to get the F1 Score Panda Array looking good\n",
    "F1Scores.set_index(score) and melt df if choose to do this, so that demographic F1s become column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** OLD STUFF FROM A PREVIOUS VERSION OF THE DATA SCRAPING ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f309e8c961be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# FOR TOTAL DATA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ficoscores.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#Necessary to rename this column for clarity of the data it represents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#because of formatting issues when parsing data from the html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Black (Bad).1'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'Hispanic (Good)'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# FOR TOTAL DATA\n",
    "data = pd.read_csv(\"ficoscores.csv\")\n",
    "#Necessary to rename this column for clarity of the data it represents\n",
    "#because of formatting issues when parsing data from the html\n",
    "data.rename(columns={'Black (Bad).1':'Hispanic (Good)'}, inplace=True)\n",
    "data.rename(columns={'Non- Hispanic white (Good)':'White (Good)'}, inplace=True)\n",
    "data.rename(columns={'Non- Hispanic white (Bad)': 'White (Bad)'}, inplace=True)\n",
    "whites = getPD('White (Good)', 'White (Bad)', data, \"white\")\n",
    "blacks = getPD('Black (Good)', 'Black (Bad)', data, \"black\")\n",
    "asians = getPD('Asian (Good)', 'Asian (Bad)', data, \"asian\")\n",
    "hispanics = getPD('Hispanic (Good)', 'Hispanic (Bad)', data, \"hispanic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-007b20bc2fbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# FOR SHORTENED DATA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Figure7A.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#Necessary to rename this column for clarity of the data it represents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#because of formatting issues when parsing data from the html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Black (Bad).1'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'Hispanic (Good)'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# FOR SHORTENED DATA\n",
    "data = pd.read_csv(\"Figure7A.csv\")\n",
    "#Necessary to rename this column for clarity of the data it represents\n",
    "#because of formatting issues when parsing data from the html\n",
    "data.rename(columns={'Black (Bad).1':'Hispanic (Good)'}, inplace=True)\n",
    "data.rename(columns={'Non- Hispanic white (Good)':'White (Good)'}, inplace=True)\n",
    "data.rename(columns={'Non- Hispanic white (Bad)': 'White (Bad)'}, inplace=True)\n",
    "whites = getPD('White (Good)', 'White (Bad)', data, \"white\")\n",
    "blacks = getPD('Black (Good)', 'Black (Bad)', data, \"black\")\n",
    "asians = getPD('Asian (Good)', 'Asian (Bad)', data, \"asian\")\n",
    "hispanics = getPD('Hispanic (Good)', 'Hispanic (Bad)', data, \"hispanic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** OLD STUFF FROM WHEN I SCRAPED DATA FROM THE WEBSITE ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "url = \"https://www.federalreserve.gov/boarddocs/rptcongress/creditscore/figtables7.htm#d7A\"\n",
    "r  = requests.get(\"https://\" +url)\n",
    "data = r.text\n",
    "soup = BeautifulSoup(data, 'lxml')\n",
    "\n",
    "\n",
    "def cell_text(cell):\n",
    "    return \" \".join(cell.stripped_strings)\n",
    "\n",
    "for table in soup.find_all('table'):\n",
    "    title = table.find('span', { 'class' : 'tablehead' }).getText()\n",
    "    subhead = table.find('span', { 'class' : 'tablesubheadsmall' }).getText()\n",
    "    fname = (title + ' - '+subhead).replace(' ', '_') + '.csv'\n",
    "    fname = fname.replace(':', '-')\n",
    "    with open(fname, 'w') as outfile:\n",
    "        output = csv.writer(outfile)\n",
    "\n",
    "        for row in table.find_all('tr'):\n",
    "            col = map(cell_text, row.find_all(re.compile('t[dh]')))\n",
    "            output.writerow(col)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#os.rename('Figure_7.D._TransRisk_Score-_Cumulative_Percentage_of_Goods_and_Bads,_by_Demographic_Group(Random-Account_Performance)_-_Race_or_ethnicity_(SSA_data).csv', 'random-account-ficoscores.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Old version of getting condensed data ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSeries(data, goodOrBad):\n",
    "    one = data[data[\"Score\"] == 20.0][goodOrBad].iloc[0]\n",
    "    two = data[data[\"Score\"] == 40.0][goodOrBad].iloc[0]\n",
    "    three = data[data[\"Score\"] == 60.0][goodOrBad].iloc[0]\n",
    "    four = data[data[\"Score\"] == 80.0][goodOrBad].iloc[0]\n",
    "    five = data[data[\"Score\"] == 100.0][goodOrBad].iloc[0]\n",
    "#     two = data[data[\"Score\"] > 20.0][data[\"Score\"] <= 40.0][goodOrBad].sum() + one\n",
    "#     three = data[data[\"Score\"] > 40.0][data[\"Score\"] <= 60.0][goodOrBad].sum() + one + two\n",
    "#     four = data[data[\"Score\"] > 60.0][data[\"Score\"] <= 80.0][goodOrBad].sum() + one + two + three\n",
    "#     five = data[data[\"Score\"] > 80.0][data[\"Score\"] <= 100.0][goodOrBad].sum() + one + two + three + four\n",
    "    return pd.Series([one, two, three, four, five])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"ficoscores.csv\")\n",
    "#Necessary to rename this column for clarity of the data it represents\n",
    "#because of formatting issues when parsing data from the html\n",
    "data.rename(columns={'Black (Bad).1':'Hispanic (Good)'}, inplace=True)\n",
    "data.rename(columns={'Non- Hispanic white (Good)':'White (Good)'}, inplace=True)\n",
    "data.rename(columns={'Non- Hispanic white (Bad)': 'White (Bad)'}, inplace=True)\n",
    "whites = getPD('White (Good)', 'White (Bad)', data, \"white\")\n",
    "blacks = getPD('Black (Good)', 'Black (Bad)', data, \"black\")\n",
    "asians = getPD('Asian (Good)', 'Asian (Bad)', data, \"asian\")\n",
    "hispanics = getPD('Hispanic (Good)', 'Hispanic (Bad)', data, \"hispanic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workspace for solution and Tutorial below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-2f3cbd0b2274>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwhite_non_default\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Score\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Non- Hispanic white (Good)\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mwhite_default\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Score\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Non- Hispanic white (Bad)\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mblack_non_default\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Score\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Black (Good)\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mblack_default\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Score\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Black (Bad)\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mhispanic_non_default\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Score\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Hispanic (Good)\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "white_non_default = data[[\"Score\", \"Non- Hispanic white (Good)\"]]\n",
    "white_default = data[[\"Score\",\"Non- Hispanic white (Bad)\"]]\n",
    "black_non_default = data[[\"Score\",\"Black (Good)\"]]\n",
    "black_default = data[[\"Score\",\"Black (Bad)\"]]\n",
    "hispanic_non_default = data[[\"Score\",\"Hispanic (Good)\"]]\n",
    "hispanic_default = data[[\"Score\",\"Hispanic (Bad)\"]]\n",
    "asian_non_default = data[[\"Score\",\"Asian (Good)\"]]\n",
    "asian_default = data[[\"Score\",\"Asian (Bad)\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getGraph(dataset, metricName, graphType):\n",
    "    i= 0\n",
    "    x = []\n",
    "    y = []\n",
    "    while(i < 100.5):\n",
    "        if(i == 72.5 or i == 77.5 or i == 92.5):\n",
    "            i = (i + 0.5)\n",
    "        curr_race_non_default = dataset[dataset[\"Score\"] >= i][metricName].sum()\n",
    "        total_race_non_default = dataset[metricName].sum()\n",
    "        yVal = curr_race_non_default / total_race_non_default\n",
    "        x.append(i)\n",
    "        y.append(yVal)\n",
    "        i = (i + 0.5)\n",
    "    plt.plot(x, y, graphType, label=metricName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing This Data:\n",
    "#### Now we have data for how many people are defaulters and non-defaulters for each score, theoretically the probability of a non-defaulter getting approved a loan ($\\hat Y$ = 1) should be the same amongst all four groups. You can see from the graph that this is not the case: a person from the black demographic group is much less likely to be approved than a white or asian non-defaulting person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "getGraph(asian_non_default, \"Asian (Good)\", 'b-')\n",
    "getGraph(white_non_default, \"Non- Hispanic white (Good)\", 'g-')\n",
    "getGraph(black_non_default, \"Black (Good)\", 'c-')\n",
    "getGraph(hispanic_non_default, \"Hispanic (Good)\", 'm-')\n",
    "plt.title(\"Probability of Non-Defaulters Getting $\\hat Y$ = 1 (Beneficial Outcome)\" )\n",
    "\n",
    "\n",
    "blue_line = mlines.Line2D([], [], color='blue', marker='.',\n",
    "                          markersize=15, label='Asian')\n",
    "green_line = mlines.Line2D([], [], color='green', marker='.',\n",
    "                          markersize=15, label='White')\n",
    "cyan_line = mlines.Line2D([], [], color='cyan', marker='.',\n",
    "                          markersize=15, label='Black')\n",
    "purple_line = mlines.Line2D([], [], color='purple', marker='.',\n",
    "                          markersize=15, label='Hispanic')\n",
    "\n",
    "plt.legend(handles=[blue_line, green_line, cyan_line, purple_line])\n",
    "\n",
    "#todo: compare the defaulters getting beneficial outcome too\n",
    "#facetgrid and seaborn - make variable 'default' or 'non-default' or 'race' for 1 panel for each race\n",
    "# recall - x\n",
    "# precision - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "asian_non_default.to_csv(\"asian-non-default.csv\")\n",
    "asian_default.to_csv(\"asian-default.csv\")\n",
    "white_non_default.to_csv(\"white-non-default.csv\")\n",
    "white_default.to_csv(\"white-default.csv\")\n",
    "black_non_default.to_csv(\"black-non-default.csv\")\n",
    "black_default.to_csv(\"black-default.csv\")\n",
    "hispanic_non_default.to_csv(\"hispanic-non-default.csv\")\n",
    "hispanic_default.to_csv(\"hispanic-default.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(asian_non_default.to_csv, open(\"asian-non-default.pkl\", \"wb\"))\n",
    "pickle.dump(asian_default, open(\"asian-default.pkl\", \"wb\"))\n",
    "pickle.dump(white_non_default, open(\"white-non-default.pkl\", \"wb\"))\n",
    "pickle.dump(white_default, open(\"white-default.pkl\", \"wb\"))\n",
    "pickle.dump(black_non_default, open(\"black-non-default.pkl\", \"wb\"))\n",
    "pickle.dump(black_default, open(\"black-default.pkl\", \"wb\"))\n",
    "pickle.dump(hispanic_non_default, open(\"hispanic-non-default.pkl\", \"wb\"))\n",
    "pickle.dump(hispanic_default, open(\"hispanic-default.pkl\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision / Recall Analysis\n",
    "\n",
    "##### Recall: Of all non-defaulters, how many did we correctly identify as non-defaulters (Gave beneficial outcome?)\n",
    "- true positives / (total false negatives and true positives)\n",
    "- correctly predicted non-defaulters / all non-defaulters\n",
    "\n",
    "##### Precision: Of the non-defaulters we predicted (given a beneficial outcome?), how many were actually non-defaulting\n",
    "- true positives / true positives + false positives\n",
    "- correctly identified non-defaulters / all predicted non-defaulters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notes:\n",
    "- true: non-defaulting (they are a good candidate for a loan)\n",
    "- good precision: good precision would mean out of the people that we predict are going to pay it, a high percentage actually are\n",
    "- poor precision: out of the people that we predict are going to pay it, a low percentage actually will\n",
    "\n",
    "*** This is why banks are looking for good precision for lower risk / cost on their part, but the non-discriminatory models attempt to put the burden of this cost on the data scientist to make more accurate models and away from the minority/protected group\n",
    "\n",
    "- recall: of the people who would pay it back, how many did we correctly identify\n",
    "- F-1 scores, can weight precision or recall depending on what is important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Analysis / Problem Space\n",
    "- Cost that accompanies low precision\n",
    "- What kinds of companies might risk placing cost on discriminated-against groups (even by accident)\n",
    "- What data sets and attributes are most commonly 'protected' and what kinds of models need to be re-trained to fit ethical platforms\n",
    "- How non-discriminatory supervised learning models can come into play on already-trained data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible Solutions\n",
    "(Non-Discriminatory Supervised Learning Models)\n",
    "- Explanation of non-discriminatory supervised learning models\n",
    "- Max Profit Classifier, Race Blind Classifier, Demographic Parity Classifier, Equal Opportunity Classifier, Equal Odds Classifier\n",
    "- Pros/Cons of using each one\n",
    "- Examples of when one might be better over another\n",
    "- Recommendations for this dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 1: Measuring Performance on Binary Classifiers\n",
    "While are many ways to calculate the performance of a binary predictor, two methods are particularly useful for fairness models:\n",
    "<ul>\n",
    "<li><i>Sensitivity</i>:\n",
    "<br/> - True Positive Rate\n",
    "<br/> - Among all of the actual 1's, what percentage did we predict were 1?\n",
    "</li>\n",
    "<li><i>Specificity</i>:\n",
    "<br/> - True Negative Rate\n",
    "<br/> - Among all of the actual 0's, what percentage did we predict were 0?\n",
    "</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getGraphData(dataset, graphType):\n",
    "    i= 0\n",
    "    x = []\n",
    "    y = []\n",
    "    while(i < 100.5):\n",
    "        # our dataset doesn't include these scores so this line is necessary\n",
    "        if(i == 72.5 or i == 77.5 or i == 92.5):\n",
    "            i = (i + 0.5)\n",
    "        # create and append the x and y values to the x and y arrays to be returned for the plot here:\n",
    "        curr_race_non_default = dataset[dataset[\"TransRisk Score\"] >= i][\"Good\"].sum()\n",
    "        total_race_non_default = dataset[\"Good\"].sum()\n",
    "        yVal = curr_race_non_default / total_race_non_default\n",
    "        x.append(i)\n",
    "        y.append(yVal)\n",
    "        i = (i + 0.5)\n",
    "    plt.plot(x, y, graphType)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps:\n",
    "\n",
    "Ideally we would use these four different values as each group’s threshold during the final decision process. As mentioned before, the Equality of Opportunity model requires the same sensitivity of all groups for its fairness requirements. Using these thresholds would satisfy those requirements, and allow us to label this predictor as Fair Under Equalized Opportunity.\n",
    "\n",
    "For our case study, the predictor that achieves fairness under Equalized Opportunity would have these thresholds for each demographic group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to Ethical Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview:\n",
    "- Measuring performance of supervised learning predictors that are:\n",
    "    - classification problems\n",
    "    - binary predictors\n",
    "- Introduction to non-discriminatory ML predictor models\n",
    "- Charting performance of a TransRisk score case study to determine if it passes as non-discriminatory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: A Brief Introduction to Non-Discriminatory Machine Learning Predictors\n",
    "For companies that use classification based predictors, sometimes the predicted outcome of individuals within a group will fully influence the decision that is made for that individual. This needs to be treated particularly carefully when the decision being made is an <i>Important Benefit</i> - ie) health care, loan approval, or college admission. What if the data that is being used to train the model is inherently discriminatory? What if factors that created the data we use was inherently discriminatory and we didn't even know? Then the outcome predicted would also be discriminatory.<br/><br/>\n",
    "This is what non-discriminatory predictors seek to solve. For example, <b>The Equal Opportunity Model</b> requires that the true positive rate for all groups in a dataset to be the same in order to achieve fairness. What does this mean in terms of performance for binary classifiers? (Write in terms of 1's an 0's below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Write Answer Here: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Introducing the TransRisk Dataset\n",
    "For this part of the tutorial, we will be working with a dataset that represents the distribution of TransRisk scores for non-defaulters (the 'Good' - people who have previously paid off their loans on time) and defaulters (the 'Bad' - people who previously haven't paid their loans on time) against four main demographic groups: Asian, Hispanic, Black, and White. Go ahead and import this data to take a look. What collected information to create TransRisk scores could be inherently discriminatory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "%matplotlib inline\n",
    "totalData = pd.read_csv(\"TransRiskScores.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For loan approval, usually a bank will set a <b>threshold TransRisk score</b> that determines who is approved and who is denied. For example, if the threshold was 60: everyone with a TransRisk score below 60 would be denied the loan, and everyone with a TransRisk score above 60 would be approved a loan.\n",
    "<br/><br/>\n",
    "How should a predictor go about deciding who should get a loan and who should not? It makes sense to say all of the people who <i>deserve</i> a loan should receive one. In the case of the TransRisk score, the group of people who <i>deserve</i> a loan would be the non-defaulters. \n",
    "<br/><br/>\n",
    "Following this logic, in theory the probability of a non-defaulter getting a loan ($\\hat Y$ = 1) at any threshold TransRisk score should be the same amongst all four groups. Finish the function below to plot the distribution of non-defaulters from one group getting ($\\hat Y$ = 1) based on a threshold value of TransRisk scores. Then, get the probabilities for all four demographic groups and plot them on top of eachother."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-2-d5a0d8158e15>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-d5a0d8158e15>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    plt.plot(x, y, graphType)\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def getGraphData(dataset, graphType):\n",
    "    dataset = dataset.set_index(\"TransRisk Score\")\n",
    "    x = []\n",
    "    y = []\n",
    "    for index, row in dataset.iterrows():\n",
    "        # create and append the x and y values to their arrays:\n",
    "        \n",
    "        \n",
    "    plt.plot(x, y, graphType)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "** Plot Graph Below **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3:  Calculating Performance\n",
    "Now that we've seen the likelihood of non-defaulting individuals from each of the four demographic groups to be approved a loan based on threshold value, let's check the performance of this model. Take a look at the original data again. This part will be easier if you can separate the data into four different dataframes, one for each demographic. <i>Note: it is also helpful if you set the index to be the TransRisk score.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Complete : Calculate the following problem for both the White and Black demographic groups with a threshold TransRisk score of 60**\n",
    "\n",
    "For all of the individuals that <i>deserve</i> a loan, how many will receive one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis: \n",
    "\n",
    "What you just calculated is the <b>sensitivity</b> of the White and Black demographic groups for a single threshold value (60). If you recall from the beginning of this tutorial, for the Equalized Opportunity fairness model the main requirement for achieving fairness is to ensure that the true positive rates (also known as the sensitivity) are the same for all groups. As we saw in our plot from Part 2, Equalized Opportunity was definitely not being satisfied. So, how might we go about finding an easy solution to this problem? The answer lies in utilizing this performance metric. To satisfy the requirement, we need to find a point where all demographic groups have the same sensitivity and the same defaulting rate. <br/><br/>\n",
    "It makes sense that sensitivity must be the same for all demographic groups (since that is what we are trying to accomplish with the <b>Equalized Opportunity Model</b>). But why do we need to have the same defaulting rate? This is because there is going to be a separate sensitivity value for each threshold we choose. It would be easy to choose a threshold score that gives us a sensitivity of 1 (the ideal sensitivity), but then what if choosing that score creates an unfair percentage of defaulters who are allowed loans for one demographic over another? That is why it is important to make sure that the percentage of defaulters is about the same for each demographic group as well, to ensure fairness.<br/><br/>\n",
    "*** How do you choose which percent default is the best? *** <br/><br/>\n",
    "This question depends on the company that is giving out the loans. Many banks will stick to an 18% loss function. Meaning that for all of the loans they give out, there's an 18% chance that the people granted a loan will default. Based on this example, you will try to find four different threshold scores (one for each demographic) that provide the same sensitivity for all, and the same 18% probability of defaulting for all.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Step 1: Visualize the Sensitivity \n",
    "<i>Create a plot representing the sensitivity and TransRisk scores to visualize the sensitivity versus the threshold score for all demographic groups</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSensitivityDF(demographic, data):\n",
    "    scores = []  \n",
    "    sensitivity = []\n",
    "    for index, row in data.iterrows():\n",
    "        # calculate sensitivity values here\n",
    "    \n",
    "    # create and return the dataframe that holds all data needed\n",
    "    # for graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot data here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Step 2: Utilize 18% Loss \n",
    "<i>Now that you have all of your sensitivity for each demographic, find the percent default for every calculation of sensitivity. </i> <br/><br/>\n",
    "*** Important Notes: ***\n",
    "- The sensitivity should be cumulative (because a threshold score means everyone at a score and above will recieve the loan)\n",
    "- The percent_default should also be cumulative\n",
    "- Is there an easy way to calculate percent default based off the calculations you've already made?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Step 3: Find the final 'fair' scores for each demographic\n",
    "<i>Now that we have found scores for each demographic that have the same percent default of 18% and the same sensitivity, write them below. These are the four threshold values that you would need to use to determine loan approval with this dataset in order to satisfy the <b>Equal Opportunity Model.</b></i> <br/><br/>\n",
    "\n",
    "*** Write Scores Here: ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have successfully taken a discriminatory set of data and utilized a machine learning fairness model to make it into a fair predictor. As our research showed, it was very obvious that the data involved in creating the supervised learning predictors for loan approval from TransRisk scores was inherently discriminatory.<br/><br/> What are some other possible solutions for optimizing performance of these models to ensure non-discriminatory decision making? \n",
    "<br/><br/>\n",
    "TransRisk data and non-discriminatory analysis courtesy of https://arxiv.org/pdf/1610.02413.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance on Ethical Supervised Learning with FICO Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview:\n",
    "- Measuring performance of machine learning models that:\n",
    "    - are classification problems\n",
    "    - have binary predictors\n",
    "- Introduction to non-discriminatory supervised learning models\n",
    "- Charting performance of a FICO score case study to determine if it passes as non-discriminatory\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Measuring Performance on Binary Classifiers\n",
    "While are many ways to calculate the performance of a binary machine learning model, two methods are particularly useful when combined:\n",
    "<ul>\n",
    "<li><i>Precision</i>:\n",
    "<br/> - Among the 1's we predict, how many were actually 1?\n",
    "</li>\n",
    "<li><i>Recall</i>:\n",
    "<br/> - Among all of the actual 1's, what percentage did we predict were 1?\n",
    "</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: A Brief Introduction to Non-Discriminatory Machine Learning Models\n",
    "For companies that use classification based machine learning models, sometimes the predicted outcome of individuals within a group will fully influence the decision that is made for that individual. This needs to be treated particularly carefully when the decision being made is a <i>Social Benefit</i> - ie) health care, loan approval, or college admission. What if the data that is being used to train the model is inherently discriminatory? What if factors that created the data we use was inherently discriminatory and we didn't even know? Then the outcome predicted would also be discriminatory.<br/><br/>\n",
    "This is what non-discriminatory models seek to solve. While there are many models to use, we will be focusing on <b>The Equal Opportunity Model</b>. This means, for each group - the true positive rate is the same. What does this mean in terms of performance for binary classifiers? (Write in terms of 1's an 0's below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Write Answer Here: **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Introducing the FICO Dataset\n",
    "For this tutorial, we will be working with a dataset that represents the distribution of FICO scores for non-defaulters (people who have previously paid off their loans on time) against four main demographic groups: Asian, Hispanic, Black, and White. Go ahead and import this data to take a look. What collected information to create FICO scores could be inherently discriminatory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "white_non_default = pickle.load(open(\"white-non-default.pkl\", \"rb\"))\n",
    "asian_non_default = pickle.load(open(\"asian-non-default.pkl\", \"rb\"))\n",
    "black_non_default = pickle.load(open(\"black-non-default.pkl\", \"rb\"))\n",
    "hispanic_non_default = pickle.load(open(\"hispanic-non-default.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theoretically, with equalized-odds the probability of a non-defaulter getting approved a loan ($\\hat Y$ = 1) should be the same amongst all four groups. Finish the function below to plot the distribution of non-defaulters from one group getting ($\\hat Y$ = 1) based on their FICO score. Then, get the probabilities for all four demographic groups and plot them on top of eachother."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getGraphData(dataset, metricName, graphType):\n",
    "    i= 0\n",
    "    x = []\n",
    "    y = []\n",
    "    while(i < 100.5):\n",
    "        # our dataset doesn't include these scores so this line is necessary\n",
    "        if(i == 72.5 or i == 77.5 or i == 92.5):\n",
    "            i = (i + 0.5)\n",
    "        # create and append the x and y values to the x and y arrays to be returned for the plot here:\n",
    "        \n",
    "        \n",
    "        \n",
    "        i = (i + 0.5)\n",
    "    plt.plot(x, y, graphType, label=metricName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "** Plot Graph Below **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Calculating Precision and Recall **<br/>\n",
    "Now that we've seen the likelihood of non-defaulting individuals from each of the four demographic groups to be approved a loan, let's check the performance of this model. First, we need to import the distribution of FICO scores for <i>defaulters</i> (people who don't historically pay loans on time) of these four groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "white_default = pickle.load(open(\"white-default.pkl\", \"rb\"))\n",
    "asian_default = pickle.load(open(\"asian-default.pkl\", \"rb\"))\n",
    "black_default = pickle.load(open(\"black-default.pkl\", \"rb\"))\n",
    "hispanic_default = pickle.load(open(\"hispanic-default.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a pandas dataframe that includes all four groups, whether they originally defaulted or not, their FICO scores, and likelihood of them getting approved a loan. Then, for each group - determine the precision score and the recall score. Are there any discrepencies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our research shows, it is very obvious that the data involved in creating the supervised learning models for loan approval from FICO scores is inherently discriminatory. What are some other possible solutions for optimizing performance of these models to ensure non-discriminatory decision making? \n",
    "<br/><br/>\n",
    "FICO data and non-discriminatory analysis courtesy of https://arxiv.org/pdf/1610.02413.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

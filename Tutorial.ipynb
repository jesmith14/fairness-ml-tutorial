{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to Fairness in Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine yourself in this scenario: You have just accepted a job at a banking company, and are tasked with the creation of a supervised learning predictor to aid in loan approval/denial. The bank has recently been under ridicule for unintentionally discriminating against certain minority groups in their loan approval process, so they want you to do some investigation for them. You are given data from the <a href='https://www.federalreserve.gov/boarddocs/rptcongress/creditscore/' target='blank'>Report to the Congress on Credit Scoring and Its Effects on the Availability and Affordability of Credit</a> in 2007 to guide your research. They want you to look at distributions of TranksRisk credit scores from 2007 in order to determine how they can fix thir process for approving and/or denying loans to people.\n",
    "\n",
    "In order to become a quick fairness expert, they take you through a quick crash-course, where you learn the following :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fairness Crash Course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. What is a fairness model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fairness models [in the context of this bank] seek to fix machine learning predictors for discrimination. Specifically, they allow data scientists to algorithmically and statistically measure how *fair* a predictor is by providing requirements to ensure *fairness*. Ususally this involves identifying minority groups within a training population distribution to ensure that predictor outcomes aren't unintentionally benefitting majority groups over minority ones. Using a fairness model's requirements, we can also determine if a predictor is *unfair* (or discriminatory)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Equal Opportunity Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This fairness model will be the easiest to utilize for your research, because the requirements for ensuring fairness directly use performance metrics easily quantified in a binary predictor. The Equal Opportunity Model takes into account three important details :\n",
    "\n",
    "1. All groups (minority and majority) are defined by a protected attribute. \n",
    "    - ** Denoted as 'A' **\n",
    "    - For your research, you will choose demographic groups as your A values\n",
    "2. The advantaged outcome group. \n",
    "    - ** Denoted as 'Y = 1' **\n",
    "    - For this scenario, an advantaged outcome group is the group of people who deserve a loan approval. We will discuss this at greater length in Part 3 of the tutorial.\n",
    "3. The binary predictor (getting granted a loan or not).\n",
    "    - ** Denoted as '$\\hat Y$ = 1' for a loan approval **\n",
    "    - ** Denoted as '$\\hat Y$ = 0' for loan denial **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Predictor is said to satisfy fairness under Equal Opportunity iff the probability of getting a positive prediction is the same for all advantaged outcome members in a protected group. <br/><br/>\n",
    "$$ P(\\hat Y = 1 | A = a, Y = 1) \\text{ is equal for ALL protected groups a.} $$\n",
    "\n",
    "*Translation: The probability that someone gets approved a loan given that they deserved a loan, is the same for all protected groups (minority and majority).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task #1 : Determine if the old method *was* discriminating against minorities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the crash course above, you have your work cut out now. Your manager tells you that she would like you to create a predictor that does the following:\n",
    "\n",
    "1. Takes in an individuals credit score (TransRisk Score) and demographic information.\n",
    "2. Spits out whether they were approved or denied a loan.\n",
    "\n",
    "She tells you that the bank's previous method of determining if someone was approved or denied simply took into account their TransRisk credit score. Anyone who had a equal to or above 23.5 was automatically approved. Anyone who had a score below 23.5 was automatically denied. The score 23.5 was their singular **Threshold Score** of choice. You decide to begin your research by understanding how this process worked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. The Old Way:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** For a threshold score x, someone is given a loan if they have a TransRisk score >= x. Using the data from the Federal Reserve Report, plot the dataframe that shows ***\n",
    "1. A vertical line through the old threshold TransRisk score, 23.5\n",
    "2. The probability of an individual receiving a loan based on their race: $ P\\text{(score} \\geq x \\text{ | race)} $\n",
    "\n",
    "*Hint:* Looking at the Report, you come across <a href='https://www.federalreserve.gov/boarddocs/rptcongress/creditscore/figtables3.htm#d3A' target='blank'>Figure 3A</a>. This indicates: $ P\\text{(score} \\leq x \\text{ | race)}$. Seems like a great starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ProbScoreLeqXGivenRace = pd.read_csv('Figure3A.csv').set_index('Score') / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've visualized how the old system worked, you start by testing if this old method was actually discriminatory or not. Remembering the *fairness* requirements from the **Equal Opportunity Model** you know that you want $ P(\\hat Y = 1 | A = a, Y = 1) \\text{ to be equal for ALL protected groups a.} $\n",
    "\n",
    "You translate this to mean that you want $ \\text{P(Score} \\geq x \\text{ | Deserved a Loan Approval, race) to be equal for ALL races} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Figure Out if Old Way Was Discriminatory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Someone *Deserves a Loan Approval* if they have historically not defaulted on their loans. Fortunately, the Federal Reserve Report gives us some great data to help us out!\n",
    "\n",
    "*Hint:* Looking at the Report, you come across <a href='https://www.federalreserve.gov/boarddocs/rptcongress/creditscore/figtables7.htm#d7A' target='blank'>Figure 7A</a>. This indicates: $ P\\text{(score} \\leq x \\text{ | Individual was historically NonDefaulting, race)}$. Seems like a great starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Task: Plot the following: ***\n",
    "1. The $ P\\text{(score} \\geq x \\text{ | NonDefault, race)}$ for each race\n",
    "2. A vertical line at the old threshold score, 23.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the $ P\\text{(score} \\geq x \\text{ | NonDefault, race)}$ the same for each race at the old threshold value of 23.5? \n",
    "\n",
    "** SPOILER: **\n",
    "It isn't! The bank *wasn't* being fair according to the **Equal Opportunity Model**. Now, it's time to go fix their mess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task #2 : Create a *Fair* Predictor for Loan Approvals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you know that choosing one threshold value as a cutoff for loan approval will **not** satisfy the Equal Opportunity Model's requirements for *fairness*. Instead, you'll have to choose a different threshold score for each race. The question now is... which thresholds should we choose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Utilizing Sensitivity\n",
    "In the last section, you plotted $ P(\\hat Y = 1 | A = a, Y = 1) $, this is also known as the **sensitivity** value. For the Equal Opportunity Model, you need the **sensitivity** to be the same for all races. Why don't you start by restructuring the data frame you created before?\n",
    "\n",
    "** Task : **\n",
    "1. Make the index the sensitivity values and the columns the TransRisk score values.\n",
    "2. Plot the new dataframe. y values = Sensitivity, x values = Scores per race.\n",
    "3. Plot a vertical line at an arbitrary sensitivity value (because we need sensitivity to be the same for all races)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can visualize what choosing a singular sensitivity value would look like: four different threshold scores, one score for each race.\n",
    "\n",
    "Your next question : ** what singular sensitivity value should you choose? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Utilizing Loss to Get The Precision Value \n",
    "In order to determine what singular senstivity value you want to choose, you'll need to do some other calculations first. For supervised learning predictors, one way to detect errors is to calculate the difference between the actual output and the predictor's output. One function used to compute the error of a predictor is called a **Loss Function**. The bank you work for calculates loss using False Positives (FP) and False Negatives (FN), and every loss value can be associated with a precision value. You are told that precision is inversely proportional to the loss that the bank is willing to accept. In order to determine what the old loss that the bank was willing to accept was, you'll need to calculate the precision for each demographic at each score, average those values to calculate a 'total precision' at each score, and then see what precision value is located at the old threshold score of 23.5. This precision value will be used in the next section.\n",
    "\n",
    "$ \\text{Precision   ==   P(Y} = 1 | \\hat Y = 1 \\text{ , race})$\n",
    "\n",
    "$ \\text{Precision   ==   P(NonDefault | Score } \\geq x \\text{ , race)} $\n",
    "\n",
    "** Your Tasks : **\n",
    "1. Calculate the precision for each race at each TransRisk score\n",
    "2. Plot this dataframe to visualize your findings. y values indicate precision, x values indicate the TransRisk score for each race.\n",
    "3. Calculate a 'total precision' value for each TransRisk score by taking the mean of the demographic precisions.\n",
    "4. Find what 'total precision' value is at a threshold score of 23.5, let's call this P.\n",
    "5. 1 - P = the loss that the bank was willing to take on, let's call this L."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Hints:*\n",
    "- Looking at the Report, you come across <a href='https://www.federalreserve.gov/boarddocs/rptcongress/creditscore/figtables6.htm#d6A' target='blank'>Figure 6A</a>. This indicates: $ P\\text{(NonDefault | Score} = x\\text{ , race). Should be helpful..}$\n",
    "\n",
    "- $ \\text{P(NonDefault | Score } \\geq x \\text{ , race)} $\n",
    "\n",
    "    $ <==> \\frac{ \\text{P(NonDefault and Score} \\geq x \\text{ , race)}}{ \\text{P(Score} \\geq x \\text{ , race)}} $\n",
    "\n",
    "    $ <==>  \\frac{ \\sum_{x^1 \\geq x} \\text{P(Score} = x^1 \\text{ , race)} * \\text {P(NonDefault | Score} = x^1 \\text{ , race)}}{\\sum_{x^1 \\geq x} \\text{P(Score} = x \\text{ , race)}} $\n",
    "\n",
    "    $ \\text{where } x \\text{ is a threshold score and } x^1 \\text{is an individual's score} $\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Task 1: Calculate the precision for each race at each TransRisk score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Plot this dataframe to visualize your findings.\n",
    "# y values indicate precision, x values indicate the TransRisk score for each race."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Task 3: Creating Total Precision Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4: Find the total precision value at TransRisk Score 23.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Task 5: Identify the P and L Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Utilizing The New Precision Value\n",
    "You have now discovered that your bank is striving for a maximum loss of L, which correlates to a precision of P. Now, we will reindex the precision data so we can merge our sensitivity dataset from section I. with this new precision dataset. First, delete the 'total precision' column you created in the previous section. You'll need to recalculate that here, since the sensitivity values are the new index.\n",
    "\n",
    "\n",
    "** Your Tasks : **\n",
    "\n",
    "1. Merge the new precision information with your dataframe from section I. (with the sensitivity values as the index). Now your old dataframe will have the sensitivity values as the index, the columns will include:\n",
    "    - the threshold scores for each race at that sensitivity\n",
    "    - the precision values for each race at that sensitivity\n",
    "2. Take the mean of all of the precision values at each sensitivity to calculate a TotalPrecision at each TransRisk score (create a new column called TotalPrecision that will have a value for each sensitivity value). This is the same process as the last section, however now we calculate a total precision for each sensitivity (since we want the sensitivity to be the same for all the demographics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task #3 : Find the Final Threshold Values for a *Fair* Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you've done all of your research. You discovered that in order to comply to the Equal Opportunity Model's requirements for fairness, you'll need four separate threshold values and one shared sensitivity value. You discovered in section II. that the sensitivity value will be the one associated with a total precision as close as possible to P. Now, all you need to do is find which sensitivity value is the index for a total precision closest to P in the new dataframe you just created. The four threshold values at that sensitivity will be your final threshold values.\n",
    "\n",
    "** Task : Determine final four threshold values to comply to Equal Opportunity Fairness Model. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The final four threshold values indicate that an individual within a demographic group will be approved a loan if their TransRisk score is equal to or above their threshold, and will be denied if it falls below.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results & Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "** *Congratulations!* **\n",
    "\n",
    "You have successfully taken your bank's discriminatory predictor and utilized a machine learning fairness model to make it into a 'fair' predictor! \n",
    "- Through visualizing the current system compared to the requirements of a fairness model, you discovered that the bank's current system was technically *unfair*.\n",
    "- The Equal Opportunity Model provided a clear requirement to ensure fairness. As you learned later, this meant that the sensitivity of the predictor had to be the same for all races. Since one threshold value resulted in four different sensitivities, you were able to determine that discrimination will occur if you don't choose one threshold per demographic.\n",
    "- Using loss minimization, you were able to use precision to find the single sensitivity that provided the least amount of loss.\n",
    "- Finally, you discovered four new threshold values to be used; one for each demographic. Ensuring fairness and fixing a discriminatory predictor.\n",
    "- Your bank is now happy **and** you are upholding ethical data science!\n",
    "<br/><br/>\n",
    "\n",
    "<center>\n",
    "* What other commonly found predictors do you think might be inherently discriminatory?*\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired by <a href='https://arxiv.org/pdf/1610.02413.pdf' target='blank'>Equality of Opportunity in Supervised Learning</a> by Hardt et. all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
